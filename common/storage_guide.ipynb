{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinkube Storage Guide\n",
    "\n",
    "Learn how to use persistent storage in the Thinkube Jupyter environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage Architecture\n",
    "\n",
    "Thinkube uses SeaweedFS for persistent storage with the following mount points:\n",
    "\n",
    "- `/home/jovyan/thinkube/notebooks/` - Your personal notebooks (100GB)\n",
    "- `/home/jovyan/thinkube/datasets/` - Shared datasets (500GB)\n",
    "- `/home/jovyan/thinkube/models/` - Shared models (200GB)\n",
    "\n",
    "All data in these directories persists across pod restarts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Storage Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Check disk usage\n",
    "def get_disk_usage(path):\n",
    "    result = subprocess.run(['df', '-h', path], capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "print(\"Notebooks storage:\")\n",
    "print(get_disk_usage('/home/jovyan/thinkube/notebooks'))\n",
    "\n",
    "print(\"\\nDatasets storage:\")\n",
    "print(get_disk_usage('/home/jovyan/thinkube/datasets'))\n",
    "\n",
    "print(\"\\nModels storage:\")\n",
    "print(get_disk_usage('/home/jovyan/thinkube/models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Notebooks\n",
    "\n",
    "Save your notebooks in `/home/jovyan/thinkube/notebooks/` for persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List your notebooks\n",
    "notebooks_dir = '/home/jovyan/thinkube/notebooks'\n",
    "for root, dirs, files in os.walk(notebooks_dir):\n",
    "    # Skip hidden directories\n",
    "    dirs[:] = [d for d in dirs if not d.startswith('.')]\n",
    "    level = root.replace(notebooks_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        if not file.startswith('.'):\n",
    "            print(f\"{sub_indent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Datasets\n",
    "\n",
    "Use `/home/jovyan/thinkube/datasets/` for datasets that should be accessible from any pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: Save a dataset to shared storage\n",
    "datasets_dir = '/home/jovyan/thinkube/datasets'\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "# Create sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'feature1': [1, 2, 3, 4, 5],\n",
    "    'feature2': [10, 20, 30, 40, 50],\n",
    "    'label': [0, 1, 0, 1, 0]\n",
    "})\n",
    "\n",
    "# Save to shared datasets\n",
    "dataset_path = os.path.join(datasets_dir, 'sample_dataset.csv')\n",
    "df.to_csv(dataset_path, index=False)\n",
    "print(f\"Dataset saved to {dataset_path}\")\n",
    "\n",
    "# Load from shared datasets\n",
    "loaded_df = pd.read_csv(dataset_path)\n",
    "print(f\"\\nLoaded dataset shape: {loaded_df.shape}\")\n",
    "print(loaded_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Models\n",
    "\n",
    "Use `/home/jovyan/thinkube/models/` for trained models accessible from any pod."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "models_dir = '/home/jovyan/thinkube/models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Train a simple model\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 4, 6, 8, 10])\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model to shared storage\n",
    "model_path = os.path.join(models_dir, 'sample_model.pkl')\n",
    "with open(model_path, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Load model from shared storage\n",
    "with open(model_path, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "print(f\"\\nLoaded model prediction for X=6: {loaded_model.predict([[6]])[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "1. **Notebooks**: Save work-in-progress in `/home/jovyan/thinkube/notebooks/`\n",
    "2. **Datasets**: Store reusable datasets in `/home/jovyan/thinkube/datasets/`\n",
    "3. **Models**: Save trained models in `/home/jovyan/thinkube/models/`\n",
    "4. **Scratch space**: Use `/home/jovyan/scratch/` for temporary large files (not persistent)\n",
    "5. **Clean up**: Remove unused files to free up space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SeaweedFS S3 Access\n",
    "\n",
    "You can also access storage via S3 API using boto3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load Thinkube environment\n",
    "load_dotenv('/home/jovyan/.thinkube_env')\n",
    "\n",
    "# Connect to SeaweedFS S3\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=os.getenv('S3_ENDPOINT'),\n",
    "    aws_access_key_id=os.getenv('S3_ACCESS_KEY'),\n",
    "    aws_secret_access_key=os.getenv('S3_SECRET_KEY')\n",
    ")\n",
    "\n",
    "# List buckets\n",
    "buckets = s3.list_buckets()\n",
    "print(\"Available S3 buckets:\")\n",
    "for bucket in buckets.get('Buckets', []):\n",
    "    print(f\"  - {bucket['Name']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
