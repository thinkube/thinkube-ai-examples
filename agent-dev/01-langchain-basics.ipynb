{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Thinkube AI Lab](../icons/tk_full_logo.svg)\n",
    "\n",
    "# LangChain Basics ðŸ¦œðŸ”—\n",
    "\n",
    "Learn the fundamentals of LangChain for building LLM applications:\n",
    "- LangChain architecture\n",
    "- Connect to LiteLLM gateway\n",
    "- Create simple chains\n",
    "- Prompt templates\n",
    "- Output parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LangChain\n",
    "\n",
    "LangChain is a framework for developing applications powered by language models. It provides:\n",
    "\n",
    "- **Chains**: Link multiple LLM calls together\n",
    "- **Prompts**: Manage and optimize prompts\n",
    "- **Memory**: Give LLMs context and history\n",
    "- **Agents**: Let LLMs make decisions and use tools\n",
    "\n",
    "On Thinkube, LangChain connects to the LiteLLM gateway for unified LLM access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to LiteLLM Gateway\n",
    "\n",
    "LiteLLM provides an OpenAI-compatible API for multiple LLM providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LangChain with LiteLLM\n",
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# TODO: Load environment variables from .thinkube_env\n",
    "# TODO: Get LITELLM_URL from environment\n",
    "# TODO: Create ChatOpenAI client pointing to LiteLLM\n",
    "# TODO: Test connection with a simple prompt\n",
    "# TODO: Display response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Simple Chain\n",
    "\n",
    "Chains link multiple steps together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple LLM chain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# TODO: Define prompt template with variable\n",
    "# TODO: Create LLMChain with llm and prompt\n",
    "# TODO: Run chain with input\n",
    "# TODO: Display output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Templates make prompts reusable and maintainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced prompt templates\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "\n",
    "# TODO: Create system message template\n",
    "# TODO: Create human message template\n",
    "# TODO: Combine into ChatPromptTemplate\n",
    "# TODO: Format with variables\n",
    "# TODO: Run and display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "Parse LLM outputs into structured data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured output parsing\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# TODO: Define Pydantic model for desired output structure\n",
    "# TODO: Create PydanticOutputParser\n",
    "# TODO: Add format instructions to prompt\n",
    "# TODO: Run chain and parse output\n",
    "# TODO: Display structured result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Composition\n",
    "\n",
    "Combine multiple chains for complex workflows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain example\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# TODO: Create first chain (e.g., generate topic)\n",
    "# TODO: Create second chain (e.g., write about topic)\n",
    "# TODO: Combine with SimpleSequentialChain\n",
    "# TODO: Run the combined chain\n",
    "# TODO: Display final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Example: Document Summarization\n",
    "\n",
    "Practical application of chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document summarization chain\n",
    "\n",
    "# TODO: Load sample document text\n",
    "# TODO: Create summarization prompt template\n",
    "# TODO: Create chain for summarization\n",
    "# TODO: Run chain on document\n",
    "# TODO: Display summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "- âœ… Use prompt templates for reusability\n",
    "- âœ… Add output parsers for structured data\n",
    "- âœ… Log prompts and outputs for debugging\n",
    "- âœ… Handle errors gracefully\n",
    "- âœ… Monitor token usage and costs\n",
    "- âœ… Use Langfuse for tracing (see 04-mlops.ipynb in common/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **02-langgraph-workflows.ipynb** - Stateful agent workflows\n",
    "- **03-nats-messaging.ipynb** - Multi-agent communication\n",
    "- **04-rag-pipeline.ipynb** - RAG with vector search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
