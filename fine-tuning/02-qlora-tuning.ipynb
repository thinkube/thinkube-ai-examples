{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Thinkube AI Lab](../icons/tk_full_logo.svg)\n",
    "\n",
    "# QLoRA Fine-Tuning ðŸŽ¯\n",
    "\n",
    "Efficient fine-tuning with QLoRA:\n",
    "- QLoRA methodology\n",
    "- 4-bit quantization\n",
    "- Adapter training\n",
    "- Merge and save\n",
    "- Best practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is QLoRA?\n",
    "\n",
    "QLoRA = Quantized Low-Rank Adaptation:\n",
    "\n",
    "- **4-bit Base Model**: Quantized to save memory\n",
    "- **LoRA Adapters**: Train small adapter layers\n",
    "- **Memory Efficient**: Fine-tune 70B models on 1 GPU\n",
    "- **Maintains Quality**: Matches full fine-tuning\n",
    "\n",
    "Key innovation: backpropagate through quantized weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with 4-bit quantization\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# TODO: Configure 4-bit quantization\n",
    "#       - load_in_4bit=True\n",
    "#       - bnb_4bit_compute_dtype=float16\n",
    "#       - bnb_4bit_quant_type=\"nf4\"\n",
    "# TODO: Load model with quantization config\n",
    "# TODO: Load tokenizer\n",
    "# TODO: Display model size and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA Adapters\n",
    "\n",
    "Setup adapter parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# TODO: Prepare model for k-bit training\n",
    "# TODO: Create LoraConfig:\n",
    "#       - r (rank): 8-64\n",
    "#       - lora_alpha: typically 2*r\n",
    "#       - target_modules: [\"q_proj\", \"v_proj\"]\n",
    "#       - lora_dropout: 0.05\n",
    "# TODO: Get PEFT model\n",
    "# TODO: Display trainable parameters percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "Format for instruction tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and format dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: Load instruction dataset (e.g., Alpaca, Dolly)\n",
    "# TODO: Apply chat template\n",
    "# TODO: Tokenize with proper padding\n",
    "# TODO: Handle max_length truncation\n",
    "# TODO: Display formatted examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Setup training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training\n",
    "from transformers import TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# TODO: Define TrainingArguments:\n",
    "#       - per_device_train_batch_size: 1-4\n",
    "#       - gradient_accumulation_steps: 4-8\n",
    "#       - num_train_epochs: 3\n",
    "#       - learning_rate: 2e-4\n",
    "#       - fp16 or bf16: True\n",
    "#       - optim: \"paged_adamw_32bit\"\n",
    "# TODO: Display configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train QLoRA Model\n",
    "\n",
    "Fine-tune with adapters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "\n",
    "# TODO: Create SFTTrainer\n",
    "# TODO: Start training\n",
    "# TODO: Monitor:\n",
    "#       - Training loss\n",
    "#       - GPU memory usage\n",
    "#       - Tokens per second\n",
    "# TODO: Save checkpoints\n",
    "# TODO: Display training stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test During Training\n",
    "\n",
    "Validate improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "\n",
    "# TODO: Prepare test prompts\n",
    "# TODO: Generate with current model\n",
    "# TODO: Compare quality\n",
    "# TODO: Display outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Adapters\n",
    "\n",
    "Combine LoRA weights with base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and save\n",
    "\n",
    "# TODO: Merge LoRA weights into base model\n",
    "# TODO: Convert back to FP16/FP32 if needed\n",
    "# TODO: Save merged model\n",
    "# TODO: Test merged model\n",
    "# TODO: Display model size comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Adapters Separately\n",
    "\n",
    "Keep adapters for flexibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "\n",
    "# TODO: Save adapter weights only\n",
    "# TODO: Save configuration\n",
    "# TODO: Document base model used\n",
    "# TODO: Test loading adapters\n",
    "# TODO: Display save location and size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Analysis\n",
    "\n",
    "Compare memory usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory comparison\n",
    "\n",
    "# TODO: Calculate memory for full fine-tuning\n",
    "# TODO: Measure actual QLoRA memory usage\n",
    "# TODO: Show memory savings\n",
    "# TODO: Display what model sizes are possible\n",
    "# TODO: Create comparison chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "- âœ… Use NF4 quantization for best results\n",
    "- âœ… Set LoRA rank based on task complexity (8-64)\n",
    "- âœ… Use paged_adamw optimizer for memory efficiency\n",
    "- âœ… Enable gradient checkpointing\n",
    "- âœ… Start with small learning rate (2e-4)\n",
    "- âœ… Save adapters separately for flexibility\n",
    "- âœ… Monitor for overfitting\n",
    "- âœ… Test merged model before deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "- Reduce batch size to 1\n",
    "- Increase gradient accumulation\n",
    "- Enable gradient checkpointing\n",
    "- Reduce sequence length\n",
    "\n",
    "### Poor Quality\n",
    "- Increase LoRA rank\n",
    "- Train for more epochs\n",
    "- Check data quality\n",
    "- Adjust learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **03-dataset-preparation.ipynb** - Create quality training data\n",
    "- **04-evaluation-deployment.ipynb** - Evaluate and deploy models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
