{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Thinkube AI Lab](../icons/tk_full_logo.svg)\n",
    "\n",
    "# Unsloth Basics ðŸš€\n",
    "\n",
    "Memory-efficient fine-tuning with Unsloth:\n",
    "- What is Unsloth\n",
    "- Setup and installation\n",
    "- Load base models\n",
    "- Quick fine-tuning example\n",
    "- Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Unsloth?\n",
    "\n",
    "Unsloth optimizes LLM fine-tuning:\n",
    "\n",
    "- **2x Faster**: Optimized kernels\n",
    "- **70% Less Memory**: Efficient implementation\n",
    "- **QLoRA Support**: 4-bit quantization\n",
    "- **Easy to Use**: Simple API\n",
    "- **Free**: Open source\n",
    "\n",
    "Perfect for fine-tuning on consumer GPUs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Unsloth (if not pre-installed)\n",
    "# !pip install unsloth\n",
    "\n",
    "# TODO: Verify Unsloth installation\n",
    "# TODO: Check CUDA availability\n",
    "# TODO: Display versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model with Unsloth\n",
    "\n",
    "Optimized model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# TODO: Choose base model (Llama-2, Mistral, etc.)\n",
    "# TODO: Set max sequence length\n",
    "# TODO: Load model and tokenizer with FastLanguageModel.from_pretrained()\n",
    "# TODO: Configure 4-bit quantization\n",
    "# TODO: Display model info and memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure LoRA Adapters\n",
    "\n",
    "Parameter-efficient fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "\n",
    "# TODO: Get PEFT model with get_peft_model()\n",
    "# TODO: Set LoRA rank and alpha\n",
    "# TODO: Choose target modules (q_proj, v_proj, etc.)\n",
    "# TODO: Display trainable parameters\n",
    "# TODO: Show parameter reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data\n",
    "\n",
    "Format data for instruction tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: Load instruction dataset\n",
    "# TODO: Format with chat template\n",
    "# TODO: Tokenize with padding\n",
    "# TODO: Create DataLoader\n",
    "# TODO: Display sample formatted example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Fine-Tuning\n",
    "\n",
    "Train with SFTTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune with SFTTrainer\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# TODO: Define training arguments\n",
    "#       - Learning rate, epochs, batch size\n",
    "#       - Gradient accumulation\n",
    "#       - Mixed precision\n",
    "# TODO: Create SFTTrainer\n",
    "# TODO: Start training\n",
    "# TODO: Monitor GPU memory\n",
    "# TODO: Display training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Fine-Tuned Model\n",
    "\n",
    "Generate with fine-tuned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation\n",
    "\n",
    "# TODO: Prepare test prompt\n",
    "# TODO: Generate response\n",
    "# TODO: Compare with base model\n",
    "# TODO: Display both outputs\n",
    "# TODO: Show improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Unsloth vs standard fine-tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark Unsloth benefits\n",
    "\n",
    "# TODO: Measure training speed (tokens/sec)\n",
    "# TODO: Compare memory usage\n",
    "# TODO: Calculate speedup\n",
    "# TODO: Display comparison chart\n",
    "# TODO: Show memory savings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n",
    "\n",
    "Save fine-tuned weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save fine-tuned model\n",
    "\n",
    "# TODO: Save LoRA adapters\n",
    "# TODO: Save full model (merged)\n",
    "# TODO: Save in different formats (GGUF, safetensors)\n",
    "# TODO: Display save locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Models\n",
    "\n",
    "Unsloth supports:\n",
    "- Llama 2, Llama 3\n",
    "- Mistral, Mixtral\n",
    "- Phi-2, Phi-3\n",
    "- Gemma\n",
    "- And more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "- âœ… Start with 4-bit quantization for memory savings\n",
    "- âœ… Use appropriate LoRA rank (8-64)\n",
    "- âœ… Monitor GPU memory usage\n",
    "- âœ… Use gradient checkpointing for large models\n",
    "- âœ… Test on small dataset first\n",
    "- âœ… Save adapters separately for flexibility\n",
    "- âœ… Compare with base model regularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **02-qlora-tuning.ipynb** - Deep dive into QLoRA\n",
    "- **03-dataset-preparation.ipynb** - Prepare quality datasets\n",
    "- **04-evaluation-deployment.ipynb** - Evaluate and deploy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
