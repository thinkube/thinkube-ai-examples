{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Thinkube AI Lab](../icons/tk_full_logo.svg)\n",
    "\n",
    "# GPU Basics ðŸŽ®\n",
    "\n",
    "Verify and understand your GPU environment:\n",
    "- Check GPU availability\n",
    "- CUDA basics\n",
    "- PyTorch GPU operations\n",
    "- Memory management\n",
    "- Multi-GPU detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why GPUs?\n",
    "\n",
    "GPUs accelerate deep learning by:\n",
    "\n",
    "- **Parallel Processing**: Thousands of cores vs CPU's dozen\n",
    "- **Matrix Operations**: Optimized for tensor math\n",
    "- **Memory Bandwidth**: Faster data transfer\n",
    "- **Speed**: 10-100x faster training\n",
    "\n",
    "Thinkube provides GPU nodes for ML workloads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU with nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run nvidia-smi to see GPU info\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch CUDA Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability in PyTorch\n",
    "import torch\n",
    "\n",
    "# TODO: Check if CUDA is available\n",
    "# TODO: Get CUDA version\n",
    "# TODO: Get number of GPUs\n",
    "# TODO: Get GPU names\n",
    "# TODO: Display GPU properties (compute capability, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Operations on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic GPU tensor operations\n",
    "import torch\n",
    "\n",
    "# TODO: Create tensor on CPU\n",
    "# TODO: Move tensor to GPU with .cuda() or .to('cuda')\n",
    "# TODO: Perform operations on GPU\n",
    "# TODO: Time CPU vs GPU operations\n",
    "# TODO: Move result back to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Memory Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor GPU memory usage\n",
    "\n",
    "# TODO: Get allocated memory\n",
    "# TODO: Get reserved memory\n",
    "# TODO: Get max memory allocated\n",
    "# TODO: Clear cache with torch.cuda.empty_cache()\n",
    "# TODO: Display memory stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark matrix multiplication\n",
    "import time\n",
    "\n",
    "# TODO: Create large matrices\n",
    "# TODO: Time matrix multiplication on CPU\n",
    "# TODO: Time matrix multiplication on GPU\n",
    "# TODO: Calculate speedup\n",
    "# TODO: Display comparison chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-GPU Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multiple GPUs\n",
    "\n",
    "# TODO: Get device count\n",
    "# TODO: List all GPU devices\n",
    "# TODO: Get properties for each GPU\n",
    "# TODO: Test operations on different GPUs\n",
    "# TODO: Display GPU topology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Management Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient memory usage\n",
    "\n",
    "# TODO: Demonstrate memory leak (forget to free)\n",
    "# TODO: Show proper cleanup with del and empty_cache()\n",
    "# TODO: Use torch.no_grad() for inference\n",
    "# TODO: Demonstrate gradient accumulation\n",
    "# TODO: Show memory profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Issues and Solutions\n",
    "\n",
    "### Out of Memory (OOM)\n",
    "- Reduce batch size\n",
    "- Use gradient accumulation\n",
    "- Enable mixed precision\n",
    "- Clear cache between runs\n",
    "\n",
    "### Slow Training\n",
    "- Check data loading (use DataLoader with workers)\n",
    "- Pin memory for faster transfers\n",
    "- Use GPU-optimized operations\n",
    "- Profile your code\n",
    "\n",
    "### CUDA Errors\n",
    "- Check tensor devices match\n",
    "- Verify CUDA compatibility\n",
    "- Restart kernel if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Continue with:\n",
    "- **02-pytorch-training.ipynb** - Train models on GPU\n",
    "- **03-distributed-training.ipynb** - Multi-GPU training\n",
    "- **04-transformers-training.ipynb** - Train transformer models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
